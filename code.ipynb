{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms \n",
    "import torch.nn as nn\n",
    "from torch.nn.init import kaiming_normal_, constant_\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612f0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    if batchNorm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.LeakyReLU(0.1,inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
    "            nn.LeakyReLU(0.1,inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_flow(in_planes):\n",
    "    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "\n",
    "\n",
    "def deconv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.LeakyReLU(0.1,inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def correlate(input1, input2):\n",
    "    out_corr = spatial_correlation_sample(input1,\n",
    "                                          input2,\n",
    "                                          kernel_size=1,\n",
    "                                          patch_size=21,\n",
    "                                          stride=1,\n",
    "                                          padding=0,\n",
    "                                          dilation_patch=2)\n",
    "    # collate dimensions 1 and 2 in order to be treated as a\n",
    "    # regular 4D tensor\n",
    "    b, ph, pw, h, w = out_corr.size()\n",
    "    out_corr = out_corr.view(b, ph * pw, h, w)/input1.size(1)\n",
    "    return F.leaky_relu_(out_corr, 0.1)\n",
    "\n",
    "\n",
    "def crop_like(input, target):\n",
    "    if input.size()[2:] == target.size()[2:]:\n",
    "        return input\n",
    "    else:\n",
    "        return input[:, :, :target.size(2), :target.size(3)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f98d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowNetS(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self,batchNorm=True):\n",
    "        super(FlowNetS,self).__init__()\n",
    "\n",
    "        self.batchNorm = batchNorm\n",
    "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
    "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
    "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
    "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
    "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
    "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
    "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
    "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
    "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
    "        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n",
    "\n",
    "        self.deconv5 = deconv(1024,512)\n",
    "        self.deconv4 = deconv(1026,256)\n",
    "        self.deconv3 = deconv(770,128)\n",
    "        self.deconv2 = deconv(386,64)\n",
    "\n",
    "        self.predict_flow6 = predict_flow(1024)\n",
    "        self.predict_flow5 = predict_flow(1026)\n",
    "        self.predict_flow4 = predict_flow(770)\n",
    "        self.predict_flow3 = predict_flow(386)\n",
    "        self.predict_flow2 = predict_flow(194)\n",
    "\n",
    "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
    "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
    "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
    "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                kaiming_normal_(m.weight, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                constant_(m.weight, 1)\n",
    "                constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv2 = self.conv2(self.conv1(x))\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
    "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
    "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
    "\n",
    "        flow6       = self.predict_flow6(out_conv6)\n",
    "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
    "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
    "\n",
    "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
    "        flow5       = self.predict_flow5(concat5)\n",
    "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
    "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
    "\n",
    "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
    "        flow4       = self.predict_flow4(concat4)\n",
    "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
    "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
    "\n",
    "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
    "        flow3       = self.predict_flow3(concat3)\n",
    "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
    "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
    "\n",
    "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
    "        flow2 = self.predict_flow2(concat2)\n",
    "\n",
    "        if self.training:\n",
    "            return flow2,flow3,flow4,flow5,flow6\n",
    "        else:\n",
    "            return flow2\n",
    "\n",
    "    def weight_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
    "\n",
    "    def bias_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'bias' in name]\n",
    "\n",
    "\n",
    "def flownets(data=None):\n",
    "    \"\"\"FlowNetS model architecture from the\n",
    "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
    "\n",
    "    Args:\n",
    "        data : pretrained weights of the network. will create a new one if not set\n",
    "    \"\"\"\n",
    "    model = FlowNetS(batchNorm=False)\n",
    "    if data is not None:\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def flownets_bn(data=None):\n",
    "    \"\"\"FlowNetS model architecture from the\n",
    "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
    "\n",
    "    Args:\n",
    "        data : pretrained weights of the network. will create a new one if not set\n",
    "    \"\"\"\n",
    "    model = FlowNetS(batchNorm=True)\n",
    "    if data is not None:\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b40c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights of FlowNetS\n",
    "model_dict = torch.load('./flownets_EPE1.951.pth.tar')\n",
    "model = flownets(model_dict)\n",
    "# model.flownet\n",
    "# model.load_state_dict(model_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8f6b1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = cv2.imread('img1.jpg',cv2.IMREAD_COLOR)\n",
    "frame2 = cv2.imread('img2.jpg',cv2.IMREAD_COLOR)\n",
    "frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "img1 = torch.from_numpy(frame1.T)\n",
    "img2 = torch.from_numpy(frame2.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "155e9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_input = torch.cat((img1,img2), dim=0)\n",
    "model_input = model_input.to(torch.float32)\n",
    "model_input = model_input.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    flow = model(model_input)\n",
    "flow_np = flow[0].detach().cpu().numpy().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aadfb329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128, 2, 1)\n",
      "(96, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "flow_np = flow_np[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a94d9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flow_lk = cv2.calcOpticalFlowFarneback(frame1_gray, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "flow_lk = cv2.calcOpticalFlowFarneback(frame1_gray, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Resize the optical flow to match the original frame size\n",
    "flow_lk = cv2.resize(flow_lk,(flow_np.shape[1],flow_np.shape[0]),interpolation=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "079f9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.799225\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((flow_np-flow_lk)**2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adefa34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
